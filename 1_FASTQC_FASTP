#!/bin/bash

#===========================================================================
# SLURM batch script to run the shotgun/capture pipeline - Step 1
# Processes multiple samples using SLURM array jobs
# Modify only the section > USER CONFIG < to match your setup
#===========================================================================

#SBATCH --job-name=1_FASTQC_FASTP                  # Job name (customize as needed)
#SBATCH --time=12:00:00                            # Total run time (adjust based on job)
#SBATCH --cpus-per-task=16                         # Number of CPUs per task
#SBATCH --mem=100G                                 # Total memory per task

#SBATCH --array=1-30%30                            # Array job range and concurrency limit (adjust as needed)

## Optional: uncomment and edit if required by your SLURM setup
##SBATCH --account=your_project_account            # SLURM account name (if required)
##SBATCH --partition=smp                           # Partition/queue name (e.g., smp, gpu)
##SBATCH --qos=12h                                  # Quality of Service (e.g., 12h, 48h)

## Optional email notifications (edit or remove)
##SBATCH --mail-user=your_email@example.com        # Email for job notifications
##SBATCH --mail-type=END                           # When to notify: BEGIN, END, FAIL, etc.

#===============================
# USER CONFIG (edit as needed)
#===============================

INDIR="path-to-raw-sequencing-data"    # Path to directory with raw reads
R1_ENDING="_R1_001.fastq.gz"           # Suffix of R1 files
R2_ENDING="_R2_001.fastq.gz"           # Suffix of R2 files
CLUMPIFY="TRUE"                        # Enable/disable clumpify step
DEDUPE="TRUE"                          # Enable/disable deduplication
FILTER="--low_complexity_filter"       # fastp filter options

#=================================
# SYSTEM CONFIG (do not modify)
#=================================

WORK=${PWD}                            # Working directory (auto-set)
OUTDIR="output"
OUT_FQC="out.fastqc"
PRE="pre"
POST="post"
OUT_CLUMPIFY="out.clumpify"
OUT_FASTP="out.fastp"
OUT_DEDUPE="out.dedupe"

FASTQC="fastqc/0.11.9"
BBTOOLS="bbmap/39.01"
FASTP="fastp/0.23.2"

CPU=${SLURM_CPUS_PER_TASK}


# prepare environment
#===================================================================
mkdir -p ${OUTDIR}/${OUT_FQC}/${PRE}
mkdir -p ${OUTDIR}/${OUT_FQC}/${POST}
mkdir -p ${OUTDIR}/${OUT_CLUMPIFY}
mkdir -p ${OUTDIR}/${OUT_FASTP}
mkdir -p ${OUTDIR}/${OUT_DEDUPE}


cd ${INDIR}
FILE_R1=$(ls *${R1_ENDING} | sed -n ${SLURM_ARRAY_TASK_ID}p)
FILE_R2=$(ls *${R2_ENDING} | sed -n ${SLURM_ARRAY_TASK_ID}p)

FILEBASE=${FILE_R1%${R1_ENDING}}

OUT_R1_CL="${FILEBASE}_clumpify_R1.fq.gz"
OUT_R2_CL="${FILEBASE}_clumpify_R2.fq.gz"

OUT_R1="${FILEBASE}_fastp_R1.fq.gz"
OUT_R2="${FILEBASE}_fastp_R2.fq.gz"

OUT_MERGED="${FILEBASE}_fastp_merged.fq.gz"

OUT_R1_DD="${FILEBASE}_fastp_dedupe__R1.fq.gz"
OUT_R2_DD="${FILEBASE}_fastp_dedupe__R2.fq.gz"

OUT_MERGED_DD="${FILEBASE}_fastp_dedupe_merged.fq.gz"



cd ${WORK}

# tasks to be performed
#===================================================================

# FASTQC PRE
#----------
module load ${FASTQC}
srun fastqc -q -o ${OUTDIR}/${OUT_FQC}/${PRE} -t 2 ${INDIR}/${FILE_R1} ${INDIR}/${FILE_R2}
module unload ${FASTQC}

# CLUMPIFY
#----------
if [ ${CLUMPIFY} == "FALSE" ]; then
	echo "Removal of read duplications using clumpify is turned off."
else
	module load ${BBTOOLS}
	srun clumpify.sh in=${INDIR}/${FILE_R1} in2=${INDIR}/${FILE_R2} out=${OUTDIR}/${OUT_CLUMPIFY}/${OUT_R1_CL} out2=${OUTDIR}/${OUT_CLUMPIFY}/${OUT_R2_CL} dedupe=t optical=f
	module unload ${BBTOOLS}
fi

# FASTP
#----------
if [ ${CLUMPIFY} == "FALSE" ]; then
	module load ${FASTP}
	srun fastp --in1 ${INDIR}/${FILE_R1} --in2 ${INDIR}/${FILE_R2} --out1 ${OUTDIR}/${OUT_FASTP}/${OUT_R1} --out2 ${OUTDIR}/${OUT_FASTP}/${OUT_R2} -m --merged_out ${OUTDIR}/${OUT_FASTP}/${OUT_MERGED} ${FILTER} -w ${CPU} --verbose --json=${OUTDIR}/${OUT_FASTP}/${FILEBASE}.json --html=${OUTDIR}/${OUT_FASTP}/${FILEBASE}.html
	module unload ${FASTP}
else
	module load ${FASTP}
	srun fastp --in1 ${OUTDIR}/${OUT_CLUMPIFY}/${OUT_R1_CL} --in2 ${OUTDIR}/${OUT_CLUMPIFY}/${OUT_R2_CL} --out1 ${OUTDIR}/${OUT_FASTP}/${OUT_R1} --out2 ${OUTDIR}/${OUT_FASTP}/${OUT_R2} -m --merged_out ${OUTDIR}/${OUT_FASTP}/${OUT_MERGED} ${FILTER} -w ${CPU} --verbose --json=${OUTDIR}/${OUT_FASTP}/${FILEBASE}.json --html=${OUTDIR}/${OUT_FASTP}/${FILEBASE}.html
	module unload ${FASTP}
fi

# DEDUPE
#----------
if [ ${DEDUPE} == "FALSE" ]; then
	echo "Removal of read duplications using dedupe is turned off."
else
	module load ${BBTOOLS}
	srun dedupe.sh in=${OUTDIR}/${OUT_FASTP}/${OUT_R1} out=${OUTDIR}/${OUT_DEDUPE}/${OUT_R1_DD} ac=f
	srun dedupe.sh in=${OUTDIR}/${OUT_FASTP}/${OUT_R2} out=${OUTDIR}/${OUT_DEDUPE}/${OUT_R2_DD} ac=f
	srun dedupe.sh in=${OUTDIR}/${OUT_FASTP}/${OUT_MERGED} out=${OUTDIR}/${OUT_DEDUPE}/${OUT_MERGED_DD} ac=f
	module unload ${BBTOOLS}
fi

# FASTQC POST
#----------
if [ ${DEDUPE} == "FALSE" ]; then
	module load ${FASTQC}
	srun fastqc -q -o ${OUTDIR}/${OUT_FQC}/${POST} -t 3 ${OUTDIR}/${OUT_FASTP}/${OUT_R1} 	${OUTDIR}/${OUT_FASTP}/${OUT_R2} ${OUTDIR}/${OUT_FASTP}/${OUT_MERGED}
	module unload ${FASTQC}
else
	module load ${FASTQC}
	srun fastqc -q -o ${OUTDIR}/${OUT_FQC}/${POST} -t 3 ${OUTDIR}/${OUT_DEDUPE}/${OUT_R1_DD} 	${OUTDIR}/${OUT_DEDUPE}/${OUT_R2_DD} ${OUTDIR}/${OUT_DEDUPE}/${OUT_MERGED_DD}
	module unload ${FASTQC}
fi
